---
title: "Glacial area"
subtitle: "[NO_GLAC_001]"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
    toc-title: Contents
    toc-depth: 3
    smooth-scroll: true
execute: 
  cache: true
author:
  - name: Anders L. Kolstad
    email: anders.kolstad@nina.no
    affiliations:
      - id: myID
        name: Norwegian Institute for Nature Research 
date: March 5, 2025
callout-icon: false
lightbox: true
css: ../../../style.css
code-links:
      - text: Add a review
        icon: github
        href: https://github.com/NINAnor/ecRxiv
bibliography: references.bib
---

<!--# This is a template for how to document the indicator analyses. Make sure also to not change the order, or modify, the headers, unless you really need to. This is because it easier to read if all the indicators are presented using the same layout. If there is one header where you don't have anything to write, just leave the header as is, and don't write anything below it. If you are providing code, be careful to annotate and comment on every step in the analysis. Before starting it is recommended to fill in as much as you can in the metadata file. This file will populate the initial table in your output.-->

<!--# Load all you dependencies here -->

```{r setup}
#| include: false
library(knitr)
library(tidyverse)
library(kableExtra)
library(here)
library(sf)
library(ggridges)
library(ggpubr)
knitr::opts_chunk$set(echo = TRUE)
path <- here::here("indicators/NO_GLAC_001")
```

```{r source}
#| echo: false
source(here::here("_common.R"))
```

```{r}
#| echo: false
meta <- readxl::read_xlsx("../metadata.xlsx")
st <- meta |>
  filter(Variable == "status") |>
  pull(Value)
version <- meta |>
  filter(Variable == "Version") |>
  pull(Value)
auth <- meta |>
  filter(Variable == "authors") |>
  pull(Value)
year <- meta |>
  filter(Variable == "yearAdded") |>
  pull(Value)
id <- meta |>
  filter(Variable == "indicatorID") |>
  pull(Value)
name <- meta |>
  filter(Variable == "indicatorName") |>
  pull(Value)
url <- meta |>
  filter(Variable == "url") |>
  pull(Value)

meta <- meta |>
  mutate(Variable = case_match(Variable,
    "indicatorID" ~ "Indicator ID" ,
    "indicatorName" ~ "Indicator Name",
    "country" ~ "Country",
    "continent" ~ "Continent",
    "ECT" ~ "Ecosystem Condition Typology Class",
    "yearAdded" ~ "Year added",
    "yearLastUpdate" ~ "Last update",
    .default = Variable
   )
  ) |>
  filter(Variable != "authors")

```

<!--# The following parts are autogenerated. Do not edit. -->

```{r}
#| echo: false
#| results: asis
status(st)
```

::: {layout-ncol="2"}
> **Recomended citation**: `r paste(auth, " ", year, ". ", name, " (ID: ", id, ") ", "v. ", version, ". ecRxiv: ", url, sep="")`

> **Version**: `r version`
:::

```{=html}
<details>
<summary>Show metadata</summary>
```

```{r tbl-meta}
#| tbl-cap: 'Indicator metadata'
#| echo: false
#| warning: false

meta |>
  select(Variable, Value) |>
  kbl(col.names = NULL) 

```

</details>

::: {.callout-tip collapse="true"}
## Logg

<!--# Update this logg with short messages for each update -->

-   01 Jan. 1901 - Original PR
:::

<hr />

<!--# Document you work below.  -->

## 1. Summary

```{=html}
<!--# 

With a maximum of 300 words, describe the indicator in general terms as you would to a non-expert. Think of this as a kind of commmon language summary. It is a good idea to include a bullet point list of the spesific steps in the workflow. Include a mention of the following aspects:

What does the metric represent?
Why is this relevant for describing ecosystem condition in this ecosystem?
What are the main anthropogenig impact factors?
What kind of data is used? 
Shortly, how is the data customized (modified, estimated, integarted) to fit its purpuse as an indicator?
What is the current status of  the metric (can it be used or is it still in development)?
How should the metric be used and interpretted, and how should it not be used/interpretted?

 -->
```

Glaciers represents both an important structural element in alpine ecosystems, providing melt water and local climate control. It is also a unique nature type in itself, with few, but very specialised, species living on or in association with the ice. Glaciers are permanently ice covered areas and they are sensitive to climate change, especially climate warming, which melts the ice. In Norway we have a rather detailed and precise map of the glacial extent from the period 1947-1985. Here we use this data as a representation of the climatic reference condition (pre climate warming), and compare the glacial extent with an update glacial extent map from 2018-2019 to create a normalised ecosystem condition indicator. The ecosystem assets are defined as intersections of bioclimatic regions and geographical regions. We have defined a threshold value (X~60~) as 10% remaining area, based on the assumption from Island Biogeography Theory that further loss of area will result in \>10% local species extinctions.

Indicator uncertainty originates from two sources. Firste we introduce a 3% error in the glacial estimates, based on the reported uncertainties in the mapping method. In practice we assign a probability distribution, rather than a point estimate, to each EA. We then resample the probability distributions for each EA 1000 times, creating 1000 estimates for the spatially aggregated indicator value. This introduces the second type of uncertainty, which is the spatial variation in indicator values across EAs.

The indicator is ready to be used. Future updates will require new maps of glacial extent in Norway, which is highly possible to produce from satellite imagery.

Interpretation: The indicator value represents the mean remaining glacial area across EAs.

Workflow

1.  Import glacial extent maps from two time periods

2.  Import delineations for bioclimatic regions and geographical regions and intersect to create strata (ecosystem assets, EA)

3.  Import alpine ecosystem mask

4.  Compile spatial dataset where each EA has data on strata category, glacial area at two time points and total mountain area.

5.  Add 3% uncertainty to glacial extent estimates

6.  Normalise variable at EA level by scaling, transforming and truncating.

7.  Resample within EAs and create a distribution of 1000 estimates of a spatially aggregate indicator value for geographical regions. Di this using two weighting schemes for comparison: weighted based on total mountain area or initial glacial area within each EA.

8.  Aggregate spatially from geographical region to national level, also using bootstrapping.

9.  Summarise national estimate and 50%CI in a table

## 2. About the underlying data

<!--# Describe the data you have used in more detail, it's origin, biases, availabilit ect.-->

### 2.1 Spatial and temporal resolution and extent

<!--# Describe the temporal and spatial resolution and extent of the data used -->

### 2.2 Original units

<!--# What are the original units for the most relevant  variables in the data-->

### 2.3 Additional comments about the dataset

<!--# Text here -->

#### 2.3.1 Instructions for citing, using and accessing data

<!--# Is the data openly available? If not, how can one access it? What are the key references to the datasets?   -->

The glaciel extent maps are distributed under CC-BY 4.0. We have made a simplified (less columns) version of the dataset available on github in order to make the workflow more reproducible.

## 3. Indicator properties

### 3.1 Ecosystem Condition Typology Class (ECT)

```{=html}
<!--# 

Describe the rationale for assigning the indicator to the ECT class. See https://oneecosystem.pensoft.net/article/58218/
This doesnt need to be very long. Maybe just a single sentence. 

-->
```

Area of glaciers is categorised as describing a C1 - Landscape and seascape characteristic. This put emphasis in the role of glaciers as a unique habitat type and a structural element in alpine ecosystems. The indicator has previously been attributed to A1 - Physical state characteristics due to the importance of melt water.

### 3.2 Ecosystem condition characteristic

```{=html}
<!--# 

Describe the ecosystem condition characteristic represented in the indicator. See 10.3897/oneeco.6.e58218 for information on what these characteristics might be.
For example, and indicator called 'Trenching in mires' could be made to represent an ecosystem characteristic 'Intact hydrology'. The term 'characteristic' is used similar to the term 'criteria' in Multiple Criteria Decition Making.  

-->
```

### 3.3 Other standards

<!--# Optional: Add text about other spesific standards, e.g. national standards, and how the indicator relates to these -->

### 3.4 Collinearities with other indicators

<!--# Describe known collinearities with other metrices (indicators or variables) that could become problematic if they were also included in the same Ecosystem Condition Assessment as the indicator described here. -->

### 3.5 Impact factors

<!--# Describe the main natural and anthropogenic factors that affecst the metric -->

## 4. Reference condition and levels

### 4.1 Reference condition

<!--# Define the reference condition (or refer to where it is defined). Note the destinction between reference condition and reference levels 10.3897/oneeco.5.e58216  -->

### 4.2 Reference levels

```{=html}
<!--# 

If relevant (i.e. if you have normalised a variable), describe the reference levels used to normalise the variable. 

Use the terminology where X~0~ referes to the referece level (the variable value) denoting the worst possible condition; X~100~denotes the optimum or best possible condition; and X~*n*~, where in is between 0 and 100, denotes any other anchoring points linking the variable scale to the indicator scale (e.g. the threshold value between good and bad condition X~60^). 

Why was the current option chosen and how were the reference levels quantified? If the reference values are calculated as part of the analyses further down, please repeat the main information here.

 -->
```

#### 4.2.1 Spatial resolution and validity

```{=html}
<!--# 

Describe the spatial resolution of the reference levels. E.g. is it defined as a fixed value for all areas, or does it vary. Also, at what spatial scale are the reference levels valid? For example, if the reference levels have a regional resolution (varies between regions), it might mean that it is only valid and correct to use for normalising local variable values that are first aggregated to regional scale. However, sometimes the reference levels are insensitive to this and can be used to scale variables at the local (e.g. plot) scale. 

 -->
```

## 5. Uncertainties

<!--# Describe the main uncertainties or sources of error in the indicator or the underlying data. -->

## 6. References

<!--# You can add references manually or use a citation manager and add intext citations as with crossreferencing and hyperlinks. See https://quarto.org/docs/authoring/footnotes-and-citations.html -->

## 7. Datasets

<!--# Describe the unique datasets seperately under seperate headers (Dataset A, Dataset B, etc.-->

### 7.1 Dataset A - Glacial extent 1947-1985

This data is based on a combination of landsat imagery and digitized topographic maps [@winsvold2014]. We use this map as our reference condition and assume this glacial extent represents the climatic period 1961-1990.

The dataset is manually downloaded.

URL: https://nve.brage.unit.no/nve-xmlui/handle/11250/2831053

```{r import1}
#| eval: false
#| code-summary: "Structure and cache data"
path1 <- paste0(path, "/data/cryoclim_GAO_NO_1947_1985_UTM_33N/cryoclim_GAO_NO_1947_1985_UTM_33N.shp")
dat_old <- sf::read_sf(path1) |>
  select(areal_km2)
saveRDS(dat_old, paste0(path, "/data/glacial_extent_old.rds"))
```

```{r readCache1}
#| code-summary: "Read data"
dat_old <- readRDS(paste0(path, "/data/glacial_extent_old.rds"))
```

### 7.2 Dataset B - Glacial extent 2018-2019

This is the map of glacial extent measured in ther period 2018-2019 [@andreassen2022].

URL: https://nve.brage.unit.no/nve-xmlui/handle/11250/2836926

```{r import2}
#| eval: false
#| code-summary: "Structure and cache data"
path2 <- paste0(path, "/data/GlacierAreaOutline_NO_2018_2019_N/GlacierAreaOutline_2018_2019_N.shp")

dat_new <- sf::read_sf(path2) |>
  select(areal_km2)
saveRDS(dat_new, paste0(path, "/data/glacial_extent_new.rds"))
```

```{r readCache2}
#| code-summary: "Read data"
dat_new <- readRDS(paste0(path, "/data/glacial_extent_new.rds"))
```

### 7.3 Dataset C - Geographical regions

Importing geographical regions.

```{r}
greg <- readRDS(paste0(path, "/data/regions.rds")) |>
  mutate(region = case_when(
    id == 3 ~ "Oestlandet",
    .default = region
  ))

```

### 7.4 Dataset D - Bioclimatic regions

Here I import a map dataset of bioclimatic sones and sections for Norway [@bakkestuen2008]. The data licence is unknown, but we have the authors permission to use the data (Bakkestuen, *pers. com.*). There is, however, a [public WMS service](https://kartkatalog.geonorge.no/metadata/66f09e22-23a4-43c6-a0df-426d2b317890), but I don't known if the data is downloadable anywhere.

I combine the sones (related to heat sums and the north-south gradient) and sections (related to oceanity and the west-east gradient) into regions. There are 24 regions, but I will combine some sones and sections to reduce this amount. Since there are few glaciers in continental areas I will combine sections 4 and 5, since we don't need that high resolution in the EAs here. Similarly, I will combine the warmer sones 1 and 2.

```{r}
#| eval: false

# file path to locally stored data
pathD <- "R:/GeoSpatialData/BiogeographicalRegions/Norway_PCA_klima/Original/20170614_Bioklima/20170614_Bioklima.gdb"

# read in the data and convert to tibbles for easy joins
soner <- sf::read_sf(pathD, layer = "Soner2017") |> 
  as_tibble() |>
  mutate(Sone = case_when(
    Sone_kode %in% c("6SO-1", "6SO-2") ~ "6SO-1-2",
    .default = Sone_kode
  ))

# unique(soner2$Sone)

seksjoner <- sf::read_sf(pathD, layer = "Seksjoner2017") |> 
  as_tibble() |>
  mutate(Seksjon = case_when(
    Seksjon_ko %in% c("6SE-4", "6SE-5") ~ "6SE-4-5",
    .default = Seksjon_ko
  ))

# unique(seksjoner$Seksjon)

# join based on SSB IDs and convert to sf object again
BCreg <- dplyr::left_join(soner , seksjoner |> select(-Shape), by = join_by(SSBID)) |>
  mutate(BCregion = paste(Seksjon, Sone)) |>
  sf::st_as_sf() |>
  select(SSBID,
  Sone,
  Seksjon,
  BCregion) |>
  st_transform(st_crs(greg))

```


```{r makeFig}
#| eval: false
#| code-summary: 'Make figure'
BCreg |>
  ggplot(aes(x = Sone, fill = BCregion)) +
  geom_bar() +
  facet_grid(Seksjon ~ Sone, scales = "free") +
  guides(fill = "none") +
  theme(axis.text.x = element_blank(),
    axis.title.x.bottom = element_blank()) +
  labs(y = "km-2")
ggsave("../img/bcreg.png")
```

```{r fig-sonerOgSeksjoner}
#| fig-cap: "Distribution of area within each combination of bioclimatic sone and section (i.e. each bioclimatic region)."

knitr::include_graphics("../img/bcreg.png")
```



```{r}
#| eval: false
BCreg <- BCreg |>
  group_by(BCregion) |>
  summarize(Shape = st_union(Shape))
saveRDS(BCreg, paste0(path, "/data/bioclimatic_regions.rds"))
```

```{r}
#| include: false
# read cache
BCreg <- readRDS(paste0(path, "/data/bioclimatic_regions.rds"))
```

### 7.5 Dataset E - Alpine ecosystem extent


<!-- This extent map might be updated in the near future. I am not sure the origin of this map. Check the ECA rapport. -->

```{r}
#| eval: false
pathE <- paste0(path, "/data/temp/alpine_extent.tif")

alpine <- stars::read_stars(pathE) |>
  st_as_sf(merge = T)

# intersections are much quicker on multipolygons, so I will union the geometries. 
alpine <- st_union(alpine)

saveRDS(alpine, paste0(path, "/data/alpine_extent.rds"))
```

```{r}
alpine <- readRDS(paste0(path, "/data/alpine_extent.rds"))
```


## 8. Spatial units

```{=html}
<!--# 

Describe the spatial units that you rely on in your analyses. Highlight the spatial units (the resolution) that the indicator values should be interpretted at. Potential spatial delineation data should eb introduced under 7.1. Datasets. We recomend using the SEEA EA terminology opf Basic Spatial Units (BSU), Ecosystem Asses (EA) and Ecosystem Accounting Area (EAA). 

-->
```


### 8.1 Ecosystem assets (EA)
Next we need to create a dataset for the EAs. The EAs will be defined as the intersection between bioclimatic regions and geographical regions

```{r}
#| eval: false
#| code-summary: 'Create EA dataset'
EA <- BCreg |>
  sf::st_intersection(greg)

# create new variable for the unique EA IDs
EA <- EA |>
  mutate(EA_ID = row_number())

saveRDS(EA, paste0(path, "/data/EAs.rds"))
```

```{r}
#| include: false
EA <- readRDS(paste0(path, "/data/EAs.rds"))
```

Then we add total mountain area as a variable to the EA dataset.

```{r mountainArea}
#| warning: false
#| code-summary: 'Intersect EAs and alpine extent map'

# get mountain area for each EA 
temp_alpine <- EA |>
  st_intersection(alpine) |>
  mutate(mountainArea = units::drop_units(st_area(Shape))) |>
  as_tibble()

#temp_alpine |>
#  group_by(BCregion) |>
#  summarise(area = sum(mountainArea)) |>
#  ggplot(aes(x = fct_reorder(BCregion, area), y = area)) +
#  geom_col() +
#  labs(x = "Bioclimatic region",
#       y = "Total mountain area (m2)") +
#  coord_flip()+
#  theme_bw()

```


Then we add the glacial area.

```{r calcCurrent}
#| eval: false
#| code-summary: 'Intersect EAs and glacier maps'
# This code block is a bit slow to run. 1 minute maybe.
dat_new_union <- st_union(dat_new)
dat_old_union <- st_union(dat_old)

temp_new <- EA |>
  st_intersection(dat_new_union) |>
  mutate(glacierAreaCurrent = units::drop_units(st_area(Shape))) |>
  as_tibble()

temp_old <- EA |>
  st_intersection(dat_old_union) |>
  mutate(glacierAreaOld = units::drop_units(st_area(Shape))) |>
  as_tibble()

saveRDS(dat_new_union, paste0(path, "/data/temp/dat_new_union.rds"))
saveRDS(dat_old_union, paste0(path, "/data/temp/dat_old_union.rds"))
saveRDS(temp_new, paste0(path, "/data/temp/temp_new.rds"))
saveRDS(temp_old, paste0(path, "/data/temp/temp_old.rds"))
```

```{r}
#| include: false

dat_new_union <- readRDS(paste0(path, "/data/temp/dat_new_union.rds"))
dat_old_union <- readRDS(paste0(path, "/data/temp/dat_old_union.rds"))
temp_new <- readRDS(paste0(path, "/data/temp/temp_new.rds"))
temp_old <- readRDS(paste0(path, "/data/temp/temp_old.rds"))
```


```{r join}
#| code-summary: 'Add glacier extent to EA dataset' 
EA <- EA |>
  as_tibble() |>
  left_join(temp_new |> select(glacierAreaCurrent,
                           EA_ID), 
            by = join_by(EA_ID)) |>
  left_join(temp_old |> select(glacierAreaOld, 
                               EA_ID),
            by = join_by(EA_ID)) |>
  left_join(temp_alpine |> select(mountainArea,
                                  EA_ID),
            by = join_by(EA_ID)) |>
  st_as_sf()

```

```{r fig-area}
#| fig-cap: 'The extent of glacier and total mountain area for each geographical region (top), bioclimatic section (middle) and sone (bottom). The area for mountains is divided by 100 to get it on the same scale as glacier area for comparisons across regions.'
#| code-summary: 'Make figure'
myVars <- c("Current",
            "Historic",
            "Mountains"
            )

one <- EA |>
  as_tibble() |>
  group_by(region) |>
  summarise(Current = sum(glacierAreaCurrent, na.rm = T),
            Historic = sum(glacierAreaOld, na.rm = T),
            Mountains = sum(mountainArea, na.rm = T)/100) |>
  pivot_longer(cols = all_of(myVars),
               names_to = "Variable",
               values_to = "Area") |>
  ggplot(aes(fill = Variable, 
             y = Area,
             x = region)) +
  geom_col(position = "dodge") +
  labs(x = "Geographical region",
       y = "Areal extent (m2) (divided by 100 in the case of mountain area)") +
  coord_flip()+
  theme_bw()

two <- EA |>
  as_tibble() |>
  mutate(seksjon = str_split_i(BCregion, " ", 1),
         sone = str_split_i(BCregion, " ", 2)) |>
  group_by(seksjon) |>
  summarise(Current = sum(glacierAreaCurrent, na.rm = T),
            Historic = sum(glacierAreaOld, na.rm = T),
            Mountains = sum(mountainArea, na.rm = T)/100) |>
  pivot_longer(cols = all_of(myVars),
               names_to = "Variable",
               values_to = "Area") |>
  ggplot(aes(fill = Variable, 
             y = Area,
             x = seksjon)) +
  geom_col(position = "dodge") +
  labs(x = "Section",
       y = "Areal extent (m2) (divided by 100 in the case of mountain area)") +
  coord_flip()+
  theme_bw()


three <- EA |>
  as_tibble() |>
  mutate(seksjon = str_split_i(BCregion, " ", 1),
         sone = str_split_i(BCregion, " ", 2)) |>
  group_by(sone) |>
  summarise(Current = sum(glacierAreaCurrent, na.rm = T),
            Historic = sum(glacierAreaOld, na.rm = T),
            Mountains = sum(mountainArea, na.rm = T)/100) |>
  pivot_longer(cols = all_of(myVars),
               names_to = "Variable",
               values_to = "Area") |>
  ggplot(aes(fill = Variable, 
             y = Area,
             x = sone)) +
  geom_col(position = "dodge") +
  labs(x = "Sones",
       y = "Areal extent (m2) (divided by 100 in the case of mountain area)") +
  coord_flip()+
  theme_bw()


ggpubr::ggarrange(one, two, three, ncol= 1)

```

We see from @fig-area that glaciers are rare in sections 4 and 5 (good reason for combining them), and sone 1-3. We could perhaps have combined sone 3 along with sones 1 and 2, but it makes little difference.
@fig-arealUnits show the summed glacial area for different spatial units. 

```{r figCurrent}
#| code-summary: 'Make maps A and B'

map1 <- EA |>
  mutate(glacierAreaCurrent = replace_na(glacierAreaCurrent, 0)) |>
  ggplot() +
  geom_sf(aes(fill = glacierAreaCurrent),
    colour = NA,
    show.legend = F
  ) +
  scale_fill_gradient(low = "skyblue", high = "dodgerblue4")  

map2 <- ggplot()+
  geom_sf(data = greg,
    fill = "skyblue",
    colour = NA) +
  geom_sf(data = dat_new,
    fill = "dodgerblue4",
    colour = NA)

# ggpubr::ggarrange(map1, map2, labels="AUTO")

```


### 8.2 Alternative EAs

In order to test the influence of the modifiable area unit problem (MAUP), 
I will define two alternative EAs: geographical region (EA_v2), and country (EAA).
In the last example, the EA and the ecosystem accounting area (EAA) is the same.

We also use the EA_v2 dataset as intermediate data when we aggregate from EAs to EAA.

#### 8.2.1 EA_v2

```{r fig-arealUnits}
#| fig-cap: 'Current glacier extent, either as the actuall occurences (A); the the summed area per ecosystem asset (B); the summed area per gegrapical region (C); or across the entire country (D). The legend is omitted for simplicity. The point here is simply to illustrate the sizes of the different spatial units, and how the minimum mapping unit affects the information the maps.'
#| warning: false
#| fig-height: 10
#| fig-width: 8
#| code-summary: 'Make maps C and D and plot'
EA_v2 <- greg |>
  mutate(greg_ID = row_number())


temp_alpine_2 <- EA_v2 |>
  st_intersection(alpine) |>
  mutate(mountainArea = units::drop_units(st_area(geometry))) |>
  as_tibble()
temp_new_2 <- EA_v2 |>
  st_intersection(dat_new_union) |>
  mutate(glacierAreaCurrent = units::drop_units(st_area(geometry))) |>
  as_tibble()
temp_old_2 <- EA_v2 |>
  st_intersection(dat_old_union) |>
  mutate(glacierAreaOld = units::drop_units(st_area(geometry))) |>
  as_tibble()

# Add data into EA_v2 dataset
EA_v2 <- EA_v2 |>
  as_tibble() |>
  left_join(
    temp_new_2 |> select(
      glacierAreaCurrent,
      greg_ID
    ),
    by = join_by(greg_ID)
  ) |>
  left_join(
    temp_old_2 |> select(
      glacierAreaOld,
      greg_ID
    ),
    by = join_by(greg_ID)
  ) |>
  left_join(
    temp_alpine_2 |> select(
      mountainArea,
      greg_ID
    ),
    by = join_by(greg_ID)
  ) |>
  st_as_sf()

map3 <- EA_v2 |>
  mutate(glacierAreaCurrent = replace_na(glacierAreaCurrent, 0)) |>
  ggplot() +
  geom_sf(aes(fill = glacierAreaCurrent),
    colour = NA,
    show.legend = F
  ) +
  scale_fill_gradient(low = "skyblue", high = "dodgerblue4")


# and the same for EAA
EAA <- readRDS(paste0(path, "/data/norway.rds")) |>
  add_column(EAA_ID = "1")


# I could get these area sums easier, but here I just copy over the method from above
temp_alpine_3 <- EAA |>
  st_intersection(alpine) |>
  mutate(mountainArea = units::drop_units(st_area(geometry))) |>
  as_tibble()
temp_new_3 <- EAA |>
  st_intersection(dat_new_union) |>
  mutate(glacierAreaCurrent = units::drop_units(st_area(geometry))) |>
  as_tibble()
temp_old_3 <- EAA |>
  st_intersection(dat_old_union) |>
  mutate(glacierAreaOld = units::drop_units(st_area(geometry))) |>
  as_tibble()

# Add data into EA_v3 dataset
EAA <- EAA |>
  as_tibble() |>
  left_join(
    temp_new_3 |> select(
      glacierAreaCurrent,
      EAA_ID
    ),
    by = join_by(EAA_ID)
  ) |>
  left_join(
    temp_old_3 |> select(
      glacierAreaOld,
      EAA_ID
    ),
    by = join_by(EAA_ID)
  ) |>
  left_join(
    temp_alpine_3 |> select(
      mountainArea,
      EAA_ID
    ),
    by = join_by(EAA_ID)
  ) |>
  st_as_sf()

map4 <- EAA |>
  #mutate(glacierAreaCurrent = replace_na(glacierAreaCurrent, 0)) |>
  ggplot() +
  geom_sf(aes(fill = glacierAreaCurrent),
    colour = NA,
    show.legend = F
  ) +
  scale_fill_gradient(low = "skyblue", high = "dodgerblue4")


ggpubr::ggarrange(
  map2, map1,  
  map3, map4, 
  labels="AUTO")

```


## 9. Analyses

Now we go on to calcuate indicator values for the Eas, and then spatially aggregting them.

### 9.1 Calculating indicator values (distributions) for the EAs

The next steps are a follows: 

1. Add 3% methodological uncertainty to the data at the EA level by sampling from a normal distribution
n numbers from a normal distribution with sd = mean*0.03.
1. Normalise the variable (the prob. dist.) at the EA level using a piecewise linear transformation.
1. Aggreate to next spatial level by sampling each probability distribution with n representative to the area weights. Try different area weightings, both by total montain area and initial glacial area.
1. Aggregate one more time to national level.


The papers describing the galcier extent data report an estimated 3% uncertainty with their method [@andreassen2022].
I assume this to mean the standard deviation is 3% of the mean. 
I start by replacing the point estimates in each EA (and altentaive EAs 2 and 3) 
with a probability function centered on the point estimate, and with sd = mean * 0.03.
In ecosystem accounting it is not common to allow uncertainty in the reference levels,
but here it is quite easy to implement, and I therefore see no reason not to.

```{r}
#| warning: false
#| code-fold: true
#| code-summary: "Replace estimates with prob. distributions"

EA <- EA |>
  # adding 3% uncertainty
  rowwise() |>
  mutate(
    glacierAreaCurrent_dist = 
      if_else(!is.na(glacierAreaCurrent),
              list(rnorm(1000, glacierAreaCurrent, glacierAreaCurrent*0.003)),
              NA), 
    glacierAreaOld_dist = 
      if_else(!is.na(glacierAreaOld),
              list(rnorm(1000, glacierAreaOld, glacierAreaOld*0.003)),
              NA))

```

Then we normalise, using a piecewise linear transformation, and get similar probability distribution for indicator values.


```{r tbl-normalise}
#| tbl-cap: 'Table of ecosystem assets, with variable values, X100 values (glacier area 1947-1985), indicator values and quartiles.'

# create a temporary dataframe
EAtemp <- EA |>
  as_tibble() |>
  # unlist and create new rows for each random pair of current and old glacier area estimate
  unnest_longer(c(
    glacierAreaCurrent_dist,
    glacierAreaOld_dist), 
    keep_empty = T) |>
  rename(variable = glacierAreaCurrent_dist, 
         x100 = glacierAreaOld_dist) |>
  mutate(
    # define x60 value
    x60 = x100*0.1,
    # normalise using piecewise linear transfomation
    indicator =  ifelse(
      variable < x60, 
        ((variable - 0)/(x60 - 0)) * 0.6, 
        ((variable - x60)/(x100 - x60)) * (1 - 0.6) + 0.6),
    # truncate
    indicator = case_when(
      indicator > 1 ~ 1,
      indicator < 0 ~ 0,
      .default = indicator
    )) |>
  # recast into list column
  drop_na(indicator) |>
  group_by(EA_ID) |>
  summarise(indicator_dist = list(indicator), .groups = "drop")

# we then paste these distributions back into the EA object

EA <- EA |>
  as_tibble() |>
  # simplify datafra by removing some columns...
  select(-glacierAreaCurrent_dist,
         -glacierAreaOld_dist,
         -GID_0,
         -NAME_0) |>
  # add the probability distribution of indicator values
  left_join(
    EAtemp,
    by = join_by(EA_ID)
  ) |>  
  # summarise the distribution
  rowwise() |>
  mutate(
    indicator = ifelse(
      !is.null(indicator_dist),
      median(unlist(indicator_dist)),
      NA),
  # add first and third quartile
    indicator_Q1 = ifelse(
      !is.null(indicator_dist),
      quantile(unlist(indicator_dist), probs = 0.25),
      NA), 
    indicator_Q3 = ifelse(
      !is.null(indicator_dist),
      quantile(unlist(indicator_dist), probs = 0.75),
      NA), 
  ) |>
  st_as_sf()

DT::datatable(EA |>
  as_tibble() |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 2))) |>
  select(
    EA_ID,
    glacierAreaCurrent,
    X100 = glacierAreaOld,
    mountainArea,
    indicator,
    indicator_Q1,
    indicator_Q3
  ))
```

@tbl-normalise shows the dataset with indicator values at the EA level. 
Also in that dataset are the actuall indicator probability distributions, which can be sampled.


### 9.2 Aggregating to geographical regions

Now we will spatially aggregate the indicator values to the level of geographic region.
We do this by sampling the probability distributions of each EA proportional to the weight.
As the weight, we we use the X~100~ value, i.e. the initial glacial area. 
EAs with no glaciers in them get NA, and are not included in the aggregated value.

For comparison, we also calcuate the aggregated values using mountain area as alternative weights, 
and we then refer to the indicator values as _v2_. 
We also try an approach where we calculate indicator values for geographical regions directly, 
that is to say, without using the oroginal EAs at all.



```{r tbl-gregAgg}
#| tbl-cap: 'Indicator values aggregated from EAs to geographical regions.'
#| code-fold: true
#| code-summary: "Sample indicator values"

# In order to control the n for how many samples we draw,
# we first calculate the summed area of glaciers and mountains

#EA |>
#  as_tibble() |>
#  summarise(total_m2 = sum(glacierAreaOld, na.rm=T)) |> 
#  pull(total_m2)
## Resul: 3 327 251 347. We can divide this by 10^4

#EA |>
#  as_tibble() |>
#  #ungroup() |>
#  summarise(total_m2 = sum(mountainArea, na.rm=T)) |> 
#  pull(total_m2)
## Result: 323 871 072 604. We can divide this by 10^6


## 
EA_to_greg <- ungroup(EA) |>
  drop_na(indicator) |>
  group_by(EA_ID, region) |>
  # sample with weights
  reframe(
    indicator = 
      list(
        sample(
          unlist(indicator_dist),
          size = glacierAreaOld*10^-4,
          replace = T)),
    indicator_v2 = 
      list(
        sample(
          unlist(indicator_dist),
          size = mountainArea*10^-6,
          replace = T))
  ) |>
  # concactenate the weighted samples
  group_by(region) |>
  summarise(indicator_dist = list(unlist(indicator)),
            indicator_v2_dist = list(unlist(indicator_v2))) |>
  # get summary values
  ungroup() |>
  rowwise() |>
  mutate(
    indicator = ifelse(
      !is.null(indicator_dist),
      median(unlist(indicator_dist)),
      NA),
  # add first and third quartile
    indicator_Q1 = ifelse(
      !is.null(indicator_dist),
      quantile(unlist(indicator_dist), probs = 0.25),
      NA), 
    indicator_Q3 = ifelse(
      !is.null(indicator_dist),
      quantile(unlist(indicator_dist), probs = 0.75),
      NA),
  # add the same for indicator_v2
    indicator_v2 = ifelse(
      !is.null(indicator_v2_dist),
      median(unlist(indicator_v2_dist)),
      NA),
  # add first and third quartile
    indicator_v2_Q1 = ifelse(
      !is.null(indicator_v2_dist),
      quantile(unlist(indicator_v2_dist), probs = 0.25),
      NA), 
    indicator_v2_Q3 = ifelse(
      !is.null(indicator_v2_dist),
      quantile(unlist(indicator_v2_dist), probs = 0.75),
      NA)
  )


DT::datatable(EA_to_greg |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 2))) |>
  select(
    -indicator_dist,
    -indicator_v2_dist
  ))
```

@tbl-gregAgg shows is the indicator values for the geographical regions. 
The estimates differ quite a lot depending onthe weights used, which we also see in @fig-ridgde1.

```{r createRidgePlot}
#| eval: false
#| code-fold: true
#| code-summary: "Create ridge plot"

# set bandwidth
bw <- 0.015

oneRidge <- EA_to_greg |>
  unnest_longer(indicator_dist) |>
  ggplot(aes(x = indicator_dist,
             y = region)) +
  geom_density_ridges(
         fill = "skyblue",
         bandwidth = bw) +
  labs(y = "",
       x = "Indicator values")+
  xlim(.55, 1)

twoRidge <- EA_to_greg |>
  unnest_longer(indicator_v2_dist) |>
  ggplot(aes(x = indicator_v2_dist,
             y = region)) +
  geom_density_ridges(
         fill = "lightgreen",
         bandwidth = bw) +
  labs(y = "",
       x = "Indicator values (alternative weights)")+
  xlim(.55, 1)

# ggpubr::ggarrange(oneRidge, twoRidge, ncol=1, labels = "auto")  

```


@fig-ridgde1 show how the weighting by the mountain area of each EA generally reduces the indicator value,
especially Midt-Norge (central Norway), where one EA (indicator value about 0.3)
had very little (but not zero) glaciers to begin with (little weight in pane a), 
but a lot of mountain area (high weight in pane b).

Now I add the spatially aggregated indicator values to the sf object with the 
geographical regions so that we can make a map.

```{r}
#| code-fold: true
#| code-summary: "Add indicators to EA_v2 dataset with the regional delineations"

EA_v2 <- EA_v2 |>
  as_tibble() |>
  left_join(EA_to_greg, by = join_by(region)) |>
  st_as_sf()
```


### 9.3. Alternative EAs (v_2) {#sec-ea2}

What if we used different spatial units as the EAs. 
Now I calculate the indicator value directly to the geographical regions. 
This indicator will be names `indicator_alt` in the code.
As we see from @fig-ridgde1 this creates smoother density plots. 
This is because the 3% error was added to the summed glacier area for the regions,
and not on the initial glacier polygons, which would also be a valid approach, 
but the central tendency should be unaffected by this.
We also see that the probabilities generally match up well with the pane a and the 
original weighting scheme.
But comparing @tbl-gregAgg and @tbl-normaliseGreg we see that the point estimates are not identical,
meaning that the two approaches are not perfectly commutative, and that some other dataset could give bigger (or smaller) differences between the two approaches than what se see here.

```{r}
#| code-fold: true
#| code-summary: "Replace glacier estimates with probability distributions"

EA_v2 <- EA_v2 |>
# adding 3% uncertainty
  rowwise() |>
  mutate(
    glacierAreaCurrent_dist = 
      if_else(!is.na(glacierAreaCurrent),
              list(rnorm(1000, glacierAreaCurrent, glacierAreaCurrent*0.003)),
              NA), 
    glacierAreaOld_dist = 
      if_else(!is.na(glacierAreaOld),
              list(rnorm(1000, glacierAreaOld, glacierAreaOld*0.003)),
              NA))
```

```{r tbl-normaliseGreg}
#| tbl-cap: 'Table with indicator values and quartiles for alternative EA approach using geographical regions.'

# create a temporary dataframe
EA_v2_temp <- EA_v2 |>
  as_tibble() |>
  # unlist and create new rows for each random pair of current and old glacier area estimate
  unnest_longer(c(
    glacierAreaCurrent_dist,
    glacierAreaOld_dist), 
    keep_empty = T) |>
  rename(variable = glacierAreaCurrent_dist, 
         x100 = glacierAreaOld_dist) |>
  mutate(
    # define x60 value
    x60 = x100*0.1,
    # normalise using piecewise linear transfomation
    indicator_alt =  ifelse(
      variable < x60, 
        ((variable - 0)/(x60 - 0)) * 0.6, 
        ((variable - x60)/(x100 - x60)) * (1 - 0.6) + 0.6),
    # truncate
    indicator_alt = case_when(
      indicator > 1 ~ 1,
      indicator < 0 ~ 0,
      .default = indicator_alt
    )) |>
  # recast into list column
  drop_na(indicator_alt) |>
  group_by(greg_ID, region) |>
  summarise(indicator_dist_alt = list(indicator_alt), .groups = "drop")

# we then paste these distributions back into the EA_v2 object

EA_v2 <- EA_v2 |>
  as_tibble() |>
  # simplify datafra by removing some columns...
  select(-glacierAreaCurrent_dist,
         -glacierAreaOld_dist,
         -GID_0,
         -NAME_0) |>
  # add the probability distribution of indicator values
  left_join(
    EA_v2_temp |> select(-region),
    by = join_by(greg_ID)
  ) |>  
  # summarise the distribution
  rowwise() |>
  mutate(
    indicator_alt = ifelse(
      !is.null(indicator_dist_alt),
      median(unlist(indicator_dist_alt)),
      NA),
  # add first and third quartile
    indicator_Q1_alt = ifelse(
      !is.null(indicator_dist_alt),
      quantile(unlist(indicator_dist_alt), probs = 0.25),
      NA), 
    indicator_Q3_alt = ifelse(
      !is.null(indicator_dist_alt),
      quantile(unlist(indicator_dist_alt), probs = 0.75),
      NA), 
  ) |>
  st_as_sf()

DT::datatable(EA_v2 |>
  as_tibble() |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 2))) |>
  select(
    greg_ID,
    region,
    #glacierAreaCurrent,
    #X100 = glacierAreaOld,
    #mountainArea,
    indicator_alt,
    indicator_Q1_alt,
    indicator_Q3_alt
  ))
```

```{r}
#| eval: false
threeRidge <- EA_v2_temp |>
  unnest_longer(indicator_dist_alt) |>
  ggplot(aes(x = indicator_dist_alt,
             y = region)) +
  geom_density_ridges(
         fill = "orchid1",
         bandwidth = bw) +
  labs(y = "",
       x = "Indicator values (alternative EA approach)") +
  xlim(.55, 1)

ggpubr::ggarrange(oneRidge, twoRidge, threeRidge, 
                  ncol=1, labels = "auto") 
ggsave(paste0(path, "/img/ridge.png"))
```

```{r fig-ridgde1}
#| include: false
#| fig-cap: 'Probability distributions for indicator values aggregated to the scale of geographical regions. a) is aggregated from the original EAs using a weighting based on the initial glacial area. b) uses a weighting based on total mountain area in each EA. c) Uses geographical regions as alternative EAs.'

knitr::include_graphics(paste0(path, "/img/ridge.png"))
```

### 9.4 Aggregating to EAA

Then we aggregate the reginal probability distributions to the entire ecosystem accounting area (EAA), 
which is mainland Norway.

```{r tbl-eaaAgg}
#| tbl-cap: 'Indicator values aggregated from geographical regions to the EAA.'
#| code-fold: true
#| code-summary: "Sample indicator values"

# In order to control the n for how many samples we draw,
# we first calculate the summed area of glaciers

#EA_v2 |>
#  as_tibble() |>
#  summarise(total_m2 = sum(glacierAreaOld, na.rm=T)) |> 
#  pull(total_m2)
## Resul: 33 327 254 334. We can divide this by 10^5


EA_v2_to_EAA <- ungroup(EA_v2) |>
  group_by(greg_ID, region) |>
  # sample with weights
  reframe(
    indicator = 
      list(
        sample(
          unlist(indicator_dist),
          size = glacierAreaOld*10^-5,
          replace = T)),
    indicator_v2 = 
      list(
        sample(
          unlist(indicator_v2_dist),
          size = glacierAreaOld*10^-5,
          replace = T)),
    indicator_alt = 
      list(
        sample(
          unlist(indicator_dist_alt),
          size = glacierAreaOld*10^-5,
          replace = T))
  ) |>
  # concactenate the weighted samples
  ungroup() |>
  summarise(indicator = list(unlist(indicator)),
            indicator_v2 = list(unlist(indicator_v2)),
            indicator_alt = list(unlist(indicator_alt))) |>
  pivot_longer(everything(),
               names_to = "Indicator_variant",
               values_to = "prob_dist") |>
  # get summary values
  rowwise() |>
  mutate(
    indicator_value = median(unlist(prob_dist)),
  # add first and third quartile
    indicator_Q1 = quantile(unlist(prob_dist), probs = 0.25), 
    indicator_Q3 = quantile(unlist(prob_dist), probs = 0.75))
  

DT::datatable(EA_v2_to_EAA |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 2))) |>
  select(
    -prob_dist
  ))
```

###9.5 Alternative EA (EAA)

Similar to in @sec-ea2 I will see what happens if I estimate the indicator value directly at the EAA level.


## 10. Results

@tbl-eaaAgg show the main results, and I recomend using the first row, i.e. the 
indicator version using the aggregation pathway 

$$sp.agg^1^ - [scale - transform - truncate] - sp-agg^2^ - sp.agg^3^$$ 

where ^1^ refers to the definition of EAs and the summation of glacial area inside each EA; 
^2^ refers to aggregation to geographical region using weighted sampling and taking the median from the resulting distribution; 
and ^3^ refers to further aggregation to the EAA, 
also using weighted sampling from the probability 
distribution of indicator values for the geographical region level.

```{r}
#| message: false

# add iondicator data to EAA 
EAA <- EAA |>
  as_tibble() |>
  right_join(EA_v2_to_EAA |> add_column(EAA_ID = "1")) |>
  st_as_sf()

# Make map of indicator values across EAs
ea_plot <- EA |>
  ggplot() +
  geom_sf(aes(fill = indicator),
    colour = NA)+
  scale_fill_gradient(na.value = "white")+
  geom_sf(data = EAA, fill = NA)

# similar map of indicator values arcross geographical regions.
# To do this I firtst need to wrnagle the data a little

EA_plotdat <- EA_v2 |>
  pivot_longer(cols = c(
    indicator,
    indicator_alt,
    indicator_v2
  ),
  names_to = "Indicator_variant",
  values_to = "indicator_value") |>
  select(
    region,
    Indicator_variant,
    indicator_value
  )

greg_plot <- EA_plotdat |>
  ggplot() +
  geom_sf(aes(fill = indicator_value),
    colour = NA) +
  facet_wrap(.~Indicator_variant)

# we need the EAA direct method here as well....
eaa_plot <- EAA |>
  ggplot() +
  geom_sf(aes(fill = indicator_value),
    colour = NA) +
  facet_wrap(.~Indicator_variant)
```





## 11. Export file

```{=html}
<!--# 

Optional: Display the code (don't execute it) or the workflow for exporting the indicator values to file. Ideally the indicator values are exported as a georeferenced shape or raster file with indicators values, reference values and errors. You can also chose to export the raw (un-normalised or unscaled variable) as a seperate product. You should not save large sptaial output data on GitHub. You can use eval=FALSE to avoid code from being executed (example below - delete if not relevant) 

-->
```

```{r export}
#| eval: false
```
