---
title: "Encroachment Meta Model"
format: 
  html:
    embed-resources: true
author:
  - name: Andrew Gray             # Enter name
    email: andrew.gray@nina.no  # Enter email
    affiliations:
      - id: myID
        name: The Norwegian Institute for Nature Research (NINA) # Enter affiliations
  - name: Ida M. Mienna                #  Enter subsequent authors like this, or remove if not relevant
    affiliations:
      - ref: myID        
  - name: Zander Venter                #  Enter subsequent authors like this, or remove if not relevant
    affiliations:
      - ref: myID              # To reuse affiliations referecen the id like this
date: October 10, 2024 # Enter date 
callout-icon: false
---

<!--# This is a template for how to document the indicator analyses. Make sure also to not change the order, or modify, the headers, unless you really need to. This is because it easier to read if all the indicators are presented using the same layout. If there is one header where you don't have anything to write, just leave the header as is, and don't write anything below it. If you are providing code, Be careful to annotate and comment on every step in the analysis. Before starting it is recommended to fill in as much as you can in the metadata file. This file will populate the initial table in your output.-->

<!--# Load all you dependencies here -->

```{r setup}
#| include: false
library(knitr)
library(kableExtra)
library(here)
library(sf)
library(tidyverse)
library(gridExtra)
library(RColorBrewer)
library(flextable)
library(dplyr)
library(ggpubr)
library(flextable)
library(parallel)
library(httr)
library(jsonlite)
library(viridisLite) 
library(patchwork)
library(magick)
knitr::opts_chunk$set(echo = TRUE)
```

```{r source}
#| echo: false
source(here::here("_common.R"))

here::here()
```

```{r}
#| echo: false
meta <- readxl::read_xlsx("../metadata.xlsx")
st <- meta |>
  filter(Variable == "status") |>
  pull(Value)
version <- meta |>
  filter(Variable == "Version") |>
  pull(Value)
auth <- meta |>
  filter(Variable == "authors") |>
  pull(Value)
year <- meta |>
  filter(Variable == "yearAdded") |>
  pull(Value)
name <- meta |>
  filter(Variable == "indicatorName") |>
  pull(Value)
url <- meta |>
  filter(Variable == "url") |>
  pull(Value)

meta <- meta |>
  mutate(Variable = case_match(Variable,
    "indicatorID" ~ "Indicator ID" ,
    "indicatorName" ~ "Indicator Name",
    "country" ~ "Country",
    "continent" ~ "Continent",
    "ECT" ~ "Ecosystem Condition Typology Class",
    "yearAdded" ~ "Year added",
    "yearLastUpdate" ~ "Last update",
    .default = Variable
   )
  ) |>
  filter(Variable != "authors")

```

<!--# The following parts are autogenertaed. Do not edit. -->

::: {layout-ncol="3"}
```{r}
#| echo: false
#| results: asis
status(st)
```

::: {.callout-note style="background: cornsilk;"}
## Recomended citation

`r paste(auth, year, ".", name, "v.", version, "ecRxiv", url, sep=" ")`
:::

::: {.callout-note style="background: khaki;"}
## Version

`r version`
:::
:::

```{=html}
<details>
<summary>Show metadata</summary>
```

```{r tbl-meta}
#| tbl-cap: 'Indicator metadata'
#| echo: false
#| warning: false

meta |>
  select(Variable, Value) |>
  kbl(col.names = NULL) 

```

```{=html}
</details>
```
# Gjengroing

<!--# Replace 'indicator name' with your the actual indicator name -->

<br />

<!--# Don't remove these three html lines -->

<br /> <br />

<hr />

<!--# Document you work below.  -->

## 1. Introduction

The Norwegian word "gjengroing" is directly translated to "regrowing" in English. The gjengroing indicator describes the regrowth of woody vegetation (trees and bushes) in open ecosystems (wetland, semi- and naturally open areas) across Norway. We will use a spatial reference approach where reference areas define good or optimal vegetation regrowth heights. We have produced an index to assess regrowth using LiDAR-based canopy height data which is documented in our NO_GJEN indicator workbook. A primary limitation of this index, however, is its reliance on Norway's national LiDAR survey, which is a static dataset with no planned resurvey (as of 2024). Here, we present a workflow to utilise airborne images to derive canopy height metrics using Meta's [High Resolution Canopy Height](https://github.com/facebookresearch/HighResCanopyHeight/tree/main) backbone model. Airborne imagery is scheduled for national resurvey every 5 to 10 years and so this implementation of the gjengroing index may be used to monitor future changes in ecosystem condition. It is worth noting that the yet-to-be-launched P-Band radar satellite, Biomass, may provide an more precise method for future monitoring once data is available.

This workbook presents the necessary steps to download image tiles, run the model (in a Jupyter Notebook), validate the data against LiDAR canopy heights, compares LiDAR and modeled index scores and discusses the limitations of this approach.


## 2. About the underlying data

We rely on the following datasets:

-   [Nature type polygons](https://kartkatalog.miljodirektoratet.no/Dataset/Details/2031) are used to identify reference areas with good ecological condition.
-   Reference values for forest and good condition were taken from our LiDAR based index approach (NO_GJEN). Future implementations of this code will derive these values using Meta's model. 
-   Moen's [bioclimatic zones](https://artsdatabanken.no/Pages/181901/Bioklimatiske_soner). We use this to stratify reference (good condition) and forest (poor condition) heights by bioclimatic (also referred to as vegetation/climatic) zones.
-   [FKB building footprints](https://kartkatalog.geonorge.no/metadata/fkb-bygning/8b4304ea-4fb0-479c-a24d-fa225e2c6e97) are used to isolate vegetation in the canopy height comparison.
-   The [SSB 10km grid](https://kartkatalog.geonorge.no/metadata/statistisk-rutenett-5000m/32ac0653-d95c-446c-8558-bf9b79f4934e) is used for visualization purposes.
-   The regional delineation for Norway (five regions) are used for aggregating and reporting gjengroing condition values.
-   Norwegian Public Roads Administration, the Norwegian Institute for Bioeconomics (NIBIO) and the National Mapping Authority's Norway in [Images aerial imagery](https://www.norgeibilder.no/). 


### 2.1 Spatial and temporal resolution

This approach processes small image tiles over selected areas of interest and in its current form uses the most recently collected airborne image. Future itereations will include the ability to select date ranges for study. Temporal resolution presented here is circa 2019 to 2024.

### 2.2 Original units

The original units for ecological condition are meters. This is the height of the vegetation within reference, polygon and forest areas.

### 2.3 Additional comments about the dataset

None.

## 3. Indicator properties

### 3.1. ECT

<!--# Describe the rationale for assigning the indicator to the ECT class -->

### 3.2. Ecosystem condition characteristic

<!--# Describe the ecosystem condition characteristic represneted in the indicator. See 10.3897/oneeco.6.e58218 for information on what these characteristics might be. -->

### 3.3. Other standards

Funksjonell sammensetning innen trofiske nivåer, reflecting the change in dominant growth form.

### 3.4. Collinearities with other indicators

There is possibly a collinearity with the [primary production indicator](#NDVI-indicator-natopen) (primærproduksjon). The primary production indicator uses the normalized difference vegetation index (NDVI) as a proxy for vegetation production. NDVI can be correlated with vegetation height and consequently yield similar results to the LiDAR-based gjengroing indicator.


## 4. Reference condition and values

### 4. 1. Reference condition

The methodology used to calculate the *gjengroing* indicator is outlined in the NO_GJEN indicator. NO_GJEN_002 is a proof of concept for using airborne imagery as an alternative to LiDAR data for producing canopy heigh maps and producing an ecosystem condition index. A full spatial analysis of the gjengroing condition is available in our original script. 


#### 4.2.3. Spatial resolution and validity

<!--# Describe the spatial resolution of the reference values. E.g. is it defined as a fixed value for all areas, or does it vary. Also, at what spatial scale is the reference values valid? For example, if the reference value has a regional resolution (varies between regions), it might mean that it is only valid and correct to use for scaling local variable values that are first aggregated to regional scale. However, sometimes the reference value is insensitive to this and can be used to scale variables at the local (e.g. plot) scale.  -->

## 5. Uncertainties

-   The Meta model performs generally worse than LiDAR on very low lying vegetation (grasses ect) and tends to overestimate their height. 
-   In some instances, mismatch was due to temporal differences between LiDAR data and image capture, where trees had been removed or planted, natural growth had occurred or datasets were from different periods within the phenological cycle. 
-   The models poor performance over shaded areas has the potential to create large uncertainty when calculating median vegetation height over an area of interest. Shade would introduce significant error when comparing images from different years based on the time of year and time of day. 
-   The LiDAR based canopy height model was better able to resolve canopy structure and small gaps within vegetation canopy. This explains the majority of points where LiDAR canopy heights were very low but modeled canopy height was high. 
-   Georectification of the two datasets was sometimes resulting in mismatch at forest/vegetation edges
-   The Meta model has visible edge effects on the output tiles. We have tried to minimise the influence of these by downloading tiles with overlapping regions, but these were also responsible for some mismatches.
-   There may be different camera specifications for different airborne surveys which could affect inference. The normalisation steps within the model should mitigate these effects but we have not tested the impact of different sensor/lens combinations.   

## 6. References

Tolan, J., Yang, H.-I., Nosarzewski, B., Couairon, G., Vo, H. V., Brandt, J., Spore, J., Majumdar, S., Haziza, D., Vamaraju, J., Moutakanni, T., Bojanowski, P., Johns, T., White, B., Tiecke, T., & Couprie, C. (2024). Very high resolution canopy height maps from RGB imagery using self-supervised vision transformer and convolutional decoder trained on aerial lidar. In Remote Sensing of Environment (Vol. 300, p. 113888). Elsevier BV. https://doi.org/10.1016/j.rse.2023.113888

## 7. Datasets

There are several datasets which are used in GEE which will not be imported into the R session here. These datasets have been obtained from the source and ingested into GEE by Zander Venter or Vegar Bakkestuen with the help of Miljødata section at NINA. They include

-   [Nasjonalt grunnkart](https://nibio.brage.unit.no/nibio-xmlui/handle/11250/3120510)
-   [LiDAR-derived digital elevation model from høydedata](https://hoydedata.no/LaserInnsyn/).
-   [FKB building footprints](https://kartkatalog.geonorge.no/metadata/fkb-bygning/8b4304ea-4fb0-479c-a24d-fa225e2c6e97)
-   [European forest clear-cut map](https://www.nature.com/articles/s41893-020-00609-y)
-   Population polygons for naturlig åpen areas (GRUK).

The remaining datasets will be imported into the R session.


### 7.1 Downloading image tiles

#### 7.1.1 Setting directories
Here we define the directories to download images and image metadata (date) from and set the output folder for downloading tiles into.  

```{r}
wms_base_url <- 'https://wms.geonorge.no/skwms1/wms.nib?SERVICE=WMS&VERSION=1.1.1&REQUEST=GetMap&SRS=EPSG:25833'
output_dir <- "data/tiles"
metadata_url <- 'https://tjenester.norgeibilder.no/rest/projectMetadata.ashx'
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}
```

#### 7.1.2 Querying the airborne imagery metadata

This enables us to determine the date of the orthophoto tiles we will later download from the geonorge webserver. 

```{r}
 query_metadata <- function(xmin, ymin, xmax, ymax, crs_code) {
  bbox_coords <- sprintf("%f,%f;%f,%f;%f,%f;%f,%f", xmin, ymin, xmin, ymax, xmax, ymax, xmax, ymin)
  
  param <- list(
    Filter = "ortofototype in (1,2,3,4,5,7,8,9,10,11,12)",
    Coordinates = bbox_coords,
    InputWKID = as.character(crs_code),
    ReturnMetadata = TRUE,
    StopOnCover = FALSE
  )
  
  response <- httr::GET(
    url = metadata_url,
    query = list(request = jsonlite::toJSON(param, auto_unbox = TRUE))
  )
  
  response_content <- httr::content(response, "text", encoding = "UTF-8")
  
  if (httr::http_status(response)$category != "Success") {
    return(NULL)
  }
  
  if (response_content == "" || is.null(response_content)) {
    return(NULL)
  }
  
  parsed_metadata <- tryCatch({
    fromJSON(response_content, flatten = TRUE)
  }, error = function(e) {
    return(NULL)
  })
  
  return(parsed_metadata)
}
```
The WMS provides the most recent image over each search area so we can pull the date from the metadata service to add to the tile filename. 

```{r}
get_most_recent_metadata <- function(metadata) {
  filtered_metadata <- metadata$ProjectMetadata %>%
    filter(!is.na(properties.fotodato_date)) %>%
    mutate(fotodato_date = as.Date(properties.fotodato_date)) %>%
    arrange(desc(fotodato_date)) %>%
    slice(1)  # Get the most recent one
  
  if (nrow(filtered_metadata) == 0) {
    return(NULL)
  }
  
  return(filtered_metadata)
}
```

#### 7.1.3  Defining the area of interest

Here we include a shapefile of our area of intertest. The script will loop through all features in the shapefile, calculate their extent and query the WMS to download orthophotos within that extent in 256 x 256 pixel tiles. The AOI shape file should be in EPSG:25833. The shapefile provided here is a random sample of NiN polygons intended for testing this approach. Tile size must be kept to 256 to enable image normalisation as part of the preprocessing for running the meta model. Model performance has been tested at 1 m and 0.5 m resolution, with significantly better results at 0.5 m resolution. Imagery is available at finer scales which may again improve performance, but may also require additional tuning of the model's hyperparameters for best results.   

```{r}

AOIs <- sf::st_read("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/AOIs.shp")

tile_size <- 256
resolution <- 0.5  # 0.5 meter resolution
```

#### 7.1.4 Downloading image tiles

This function downloads the orthophoto image tiles over our areas of interest. 
```{r}
fn.getOrtoImages <- function(geometry, id_field, output_dir) {
  tryCatch({
    if (!st_is_valid(geometry)) {
      geometry <- st_make_valid(geometry)
    }
    
    geom_extent <- st_bbox(geometry)
    
    # Query metadata to get the most recent layer and date
    metadata <- query_metadata(geom_extent$xmin, geom_extent$ymin, geom_extent$xmax, geom_extent$ymax, 25833)
    
    if (is.null(metadata) || is.null(metadata$ProjectMetadata)) {
      return(NULL)
    }
    
    recent_project <- get_most_recent_metadata(metadata)
    
    if (is.null(recent_project)) {
      return(NULL)
    }
    
    recent_layer <- recent_project$properties.prosjektnavn
    image_date <- recent_project$properties.fotodato_date
    
    tile_width_meters <- tile_size * resolution
    tile_height_meters <- tile_size * resolution
    
    # Ensure tile size remains 256x256
    x_min <- floor(geom_extent$xmin / tile_width_meters) * tile_width_meters
    y_min <- floor(geom_extent$ymin / tile_height_meters) * tile_height_meters
    x_max <- ceiling(geom_extent$xmax / tile_width_meters) * tile_width_meters
    y_max <- ceiling(geom_extent$ymax / tile_height_meters) * tile_height_meters
    
    # Calculate number of tiles in x and y directions
    x_tiles <- ceiling((x_max - x_min) / tile_width_meters)
    y_tiles <- ceiling((y_max - y_min) / tile_height_meters)
    
    tile_index <- 1
    
    for (i in 0:(x_tiles - 1)) {
      for (j in 0:(y_tiles - 1)) {
        xmin <- x_min + i * tile_width_meters
        xmax <- xmin + tile_width_meters
        ymin <- y_min + j * tile_height_meters
        ymax <- ymin + tile_height_meters
        
        # Ensure the tiles have consistent size
        bbox_polygon <- st_polygon(list(matrix(c(
          xmin, ymin,
          xmin, ymax,
          xmax, ymax,
          xmax, ymin,
          xmin, ymin
        ), ncol = 2, byrow = TRUE)))
        bbox_polygon <- st_sfc(bbox_polygon, crs = st_crs(geometry))
        
        bbx <- st_bbox(bbox_polygon)
        
        # Construct WMS URL 
        wms_url <- paste0(
          'https://wms.geonorge.no/skwms1/wms.nib?',
          'SERVICE=WMS&VERSION=1.1.1&REQUEST=GetMap',
          '&LAYERS=', URLencode(recent_layer),
          '&SRS=EPSG:25833',
          '&BBOX=', paste(bbx[c("xmin", "ymin", "xmax", "ymax")], collapse = ","),
          '&WIDTH=', tile_size,
          '&HEIGHT=', tile_size,
          '&FORMAT=image/tiff',
          '&TRANSPARENT=TRUE'
        )
        
        # Output file path using the "id" field, tile index, and image date in the file name
        t <- file.path(output_dir, paste0(image_date,"_", id_field, "_", tile_index, ".tif"))
        
        # GDAL translate options
        reso <- c('-tr', as.character(resolution), as.character(resolution), '-co', 'COMPRESS=NONE')
        
        # Try downloading the tile
        result <- try({
          sf::gdal_utils('translate', source = wms_url, destination = t, options = reso)
        }, silent = TRUE)
        
        # Retry mechanism
        for (k in 1:5) {
          if (!file.exists(t)) {
            Sys.sleep(1)
            result <- try({
              sf::gdal_utils('translate', source = wms_url, destination = t, options = reso)
            }, silent = TRUE)
          }
        }
        
        tile_index <- tile_index + 1
      }
    }
    
  }, error = function(e) {
    return(NULL)
  })
}
```

We can run the functions in parallel to speed up processing time adjust the number of cores to suit your setup.
```{r}
#| eval: false
numCores <- 30  

# Use mclapply for parallel processing
mclapply(1:nrow(AOIs), function(i) {
  geometry <- AOIs[i, ]
  id_field <- AOIs$id[i]  # Adjust field name if necessary
  fn.getOrtoImages(geometry, id_field, output_dir)
}, mc.cores = numCores)
```


### 7.3 Regions {#reg-enc}

To use our model output as an indicator, we need to bring in some datasets to stratify our inferred canopy height statistics. The regional delineation for Norway (five regions) are used for aggregating and reporting gjengroing condition values.

```{r}
regions <- sf::st_read("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/regions.shp", options = "ENCODING=UTF8") %>%
  mutate(region = factor(region))
```

### 7.4 Bioclimatic regions

The [Moen's bioclimatic regions](https://data.artsdatabanken.no//Natur_i_Norge/Natursystem/Beskrivelsessystem/Regional_naturvariasjon/Bioklimatisk_sone) are imported from NINA R/GeoSpatialData/ server.

```{r}
bioclim <- st_read('/data/R/GeoSpatialData/BiogeographicalRegions/Norway_VegetationZones_Moen/Original/Vector/soner.shp') %>%
  mutate(NAVN = ifelse(NAVN == 'S°rboreal', 'Sørboreal', NAVN))
bioclim <- st_transform(bioclim, st_crs(regions))
```


## 8. Spatial units

Image tiles must be 256 x 256 pixels to pass into the model for inference. We have tested the model using the compressed_SSLhuge_aerial.pth checkpoint on 1 m and 0.5 m pixel resolution and found 0.5 m to perform better.   

## 9. Analyses

### 9.1 Modeling canopy height 

Now that we have our tiles, we can pass them to the canopy height model. Please see [Meta's High Resolution Canopy Height Model](https://github.com/facebookresearch/HighResCanopyHeight/tree/main) and Tolan et al. (2024) for more details on the model. Install the model according to the instructions on the facebookresearch github page. 

The following is python code to run on the downloaded image tiles.  


```{python}
#| eval: false

#Import dependencies

import argparse
import os
import torch
import torchvision.transforms as T
import matplotlib.pyplot as plt
from pathlib import Path
import torch.nn as nn
from tqdm import tqdm
from PIL import Image
import torchvision.transforms.functional as TF
from torchvision.utils import save_image

from models.backbone import SSLVisionTransformer
from models.dpt_head import DPTHead
import pytorch_lightning as pl
import rasterio
from rasterio.transform import from_origin

import torch
import torchvision.transforms.functional as TF
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import rasterio
from models.regressor import RNet
import torch.nn.functional as F

# New function to blend overlapping regions between adjacent tiles
def blend_overlap(region1, region2, weight=0.5):
    return (weight * region1 + (1 - weight) * region2)

# Perform histogram normalization using the pretrained model with overlap adjustment
def normalize_image(image, model_norm, device, overlap=None, weight=0.5):
    print(f"Original image shape: {image.shape}")

    if image.shape[0] != 3:
        raise ValueError(f"Expected 3-channel image, but got {image.shape[0]} channels.")

    x = torch.unsqueeze(image, dim=0).to(device)

    # Use model to predict p5 and p95 values
    norm_img = model_norm(x).detach()

    p5I = [norm_img[0][0].item(), norm_img[0][1].item(), norm_img[0][2].item()]
    p95I = [norm_img[0][3].item(), norm_img[0][4].item(), norm_img[0][5].item()]

    p5In = [np.percentile(image[i, :, :].numpy(), 20) for i in range(3)]
    p95In = [np.percentile(image[i, :, :].numpy(), 80) for i in range(3)]

    normIn = image.clone()
    
    for i in range(3):
        normIn[i, :, :] = (image[i, :, :] - p5In[i]) * ((p95I[i] - p5I[i]) / (p95In[i] - p5In[i])) + p5I[i]
    
    # If overlap region is provided, blend the overlap
    if overlap is not None:
        for i in range(3):
            overlap_region = overlap[i, :, :]
            normIn[i, :, :] = blend_overlap(normIn[i, :, :], overlap_region, weight=weight)

    return normIn


# Apply final normalization using the general image normalization values
def final_normalize(image, norm_values):
    norm = T.Normalize(*norm_values)
    return norm(image)

# Model Definitions
class SSLAE(nn.Module):
    def __init__(self, pretrained=None, classify=True, n_bins=256, huge=False):
        super().__init__()
        if huge:
            self.backbone = SSLVisionTransformer(
                embed_dim=1280,
                num_heads=20,
                out_indices=(9, 16, 22, 29),
                depth=32,
                pretrained=pretrained
            )
            self.decode_head = DPTHead(
                classify=classify,
                in_channels=(1280, 1280, 1280, 1280),
                embed_dims=1280,
                post_process_channels=[160, 320, 640, 1280],
            )
        else:
            self.backbone = SSLVisionTransformer(pretrained=pretrained)
            self.decode_head = DPTHead(classify=classify, n_bins=256)

    def forward(self, x):
        x = self.backbone(x)
        x = self.decode_head(x)
        return x

class SSLModule(pl.LightningModule):
    def __init__(self, ssl_path="compressed_SSLbaseline.pth"):
        super().__init__()

        if 'huge' in ssl_path:
            self.chm_module_ = SSLAE(classify=True, huge=True).eval()
        else:
            self.chm_module_ = SSLAE(classify=True, huge=False).eval()

        if 'compressed' in ssl_path:
            ckpt = torch.load(ssl_path, map_location='cpu')
            self.chm_module_ = torch.quantization.quantize_dynamic(
                self.chm_module_,
                {torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d},
                dtype=torch.qint8)
            self.chm_module_.load_state_dict(ckpt, strict=False)
        else:
            ckpt = torch.load(ssl_path)
            state_dict = ckpt['state_dict']
            self.chm_module_.load_state_dict(state_dict)
   
        self.chm_module = lambda x: 9.51*self.chm_module_(x) + 0.4

    def forward(self, x):
        x = self.chm_module(x)
        return x


def evaluate(model, model_norm, norm_values, name, bs=32, device='cuda:0', display=False):
    print("Starting evaluation...")

    ds = NorwayDataset(src_img_dir='data/tiles') # Set to the directory with the tiles in it


    print(f"Number of images found: {len(ds)}")

    dataloader = torch.utils.data.DataLoader(ds, batch_size=bs, shuffle=False, num_workers=8, collate_fn=lambda x: tuple(zip(*x)))

    for batch in tqdm(dataloader):
        images, img_paths = batch

        # Convert list of images to a single tensor
        images = torch.stack(images).to(device)

        # Ensure the dimensions are in the correct order (channels first)
        images = images.permute(0, 2, 3, 1)  

        # Step 1: Apply histogram normalization (matching satellite images)
        images = torch.stack([normalize_image(img, model_norm, device) for img in images])

        # Step 2: Apply final normalization (standard image normalization)
        images = torch.stack([final_normalize(img, norm_values) for img in images])

        pred = model(images)
        pred = pred.cpu().detach().relu()

        for ind in range(pred.shape[0]):
            img_path = img_paths[ind]
            print(f"Processing {img_path}")

            with rasterio.open(img_path) as src:
                meta = src.meta.copy()
                transform = src.transform 
                crs = src.crs  

            meta.update(dtype=rasterio.float32, count=1)

            # Save the prediction as GeoTIFF
            output_path = Path(name) / f'{img_path.stem}_pred.tif'
            with rasterio.open(output_path, 'w', **meta) as dst:
                dst.write(pred[ind].numpy()[0], 1)  # Writing the single prediction band
                dst.transform = transform  # Ensure the correct geotransform is used
                dst.crs = crs  # Ensure the correct CRS is used

            if display:
                plt.imshow(pred[ind][0].numpy())
                plt.title(f'Prediction for {img_path.name}')
                plt.show()


# Dataset Definition
class NorwayDataset(torch.utils.data.Dataset):
    def __init__(self, src_img_dir='data/tiles'): #make sure the correct tile directory is used
        self.src_img_dir = Path(src_img_dir)
        self.img_files = list(self.src_img_dir.glob('*.tif'))  

    def __len__(self):
        return len(self.img_files)

    def __getitem__(self, i):
        img_path = self.img_files[i]
        print(f"Loading image: {img_path}")
        
        # Use rasterio to read the GeoTIFF
        with rasterio.open(img_path) as src:
            img = TF.to_tensor(src.read([1, 2, 3]))  
        
        # Check if the image is all zeros
        if torch.all(img == 0):
            print(f"Skipping all-zero tile: {img_path}")
            return self.__getitem__((i + 1) % len(self.img_files))  # Get the next image

        return img, img_path

# Argument Parsing and Main
def parse_args():
    parser = argparse.ArgumentParser(description='test a model')
    parser.add_argument('--checkpoint', type=str, help='CHM pred checkpoint file', default='saved_checkpoints/compressed_SSLhuge_aerial.pth') # this should be set to the location where the model was installed
    parser.add_argument('--name', type=str, help='run name', default='data/output_inference')
    parser.add_argument('--display', type=bool, help='saving outputs in images')
    args = parser.parse_args()
    return args

def main():
    args = parse_args()
    if 'compressed' in args.checkpoint:
        device = 'cpu'
    else:
        device = 'cpu'

    os.makedirs(args.name, exist_ok=True)

    # Load SSL model
    model = SSLModule(ssl_path=args.checkpoint)
    model.to(device)
    model = model.eval()

    # Load normalization model (RNet for histogram matching)
    norm_path = 'saved_checkpoints/aerial_normalization_quantiles_predictor.ckpt'
    ckpt = torch.load(norm_path, map_location=device)
    model_norm = RNet(n_classes=6).to(device).eval()
    state_dict = ckpt['state_dict']
    for k in list(state_dict.keys()):
        if 'backbone.' in k:
            new_k = k.replace('backbone.', '')
            state_dict[new_k] = state_dict.pop(k)
    model_norm.load_state_dict(state_dict)

    # Image normalization values
    norm_values = ((0.420, 0.411, 0.296), (0.213, 0.156, 0.143))

    evaluate(model=model, model_norm=model_norm, norm_values=norm_values, name=args.name, bs=16, device=device, display=args.display)

if __name__ == '__main__':
    main()

```


### 9.2 Calculating the gjengroing index

#### 9.2.1 Importing the data

Here we import a shapefile of the areas of interest used to download image tiles. QGIS has been used to perform zonal statistics to calculate the median and maximum canopy height, calculated from both Meta model inference and LiDAR data, for each NiN feature.  

```{r}
metaChm <- st_read("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/NiN_metaTest.shp")
```

#### 9.2.1 Adding reference information

Here we add the region and bioclimate zone information to stratifiy the dataset

```{r}
library(dplyr)

# Calculate the intersection between polygons and climatic zones
main_climatic_overlap <- st_intersects(metaChm, bioclim)

# Extract the modal climatic zone for each polygon
metaChm$bioclim <- sapply(main_climatic_overlap, function(zones) {
  if (length(zones) > 0) {
    # Get the climatic zone IDs for overlapping polygons
    zone_ids <- bioclim$KLASSE[zones]
    
    # Find the most common climatic zone (modal value)
    zone_modal <- names(sort(table(zone_ids), decreasing = TRUE))[1]
    
    return(zone_modal)
  } else {
    return(NA)  # No overlap case
  }
})

# Calculate the intersection between polygons and regions
main_region_overlap <- st_intersects(metaChm, regions)

# Extract the modal (most common) region for each polygon
metaChm$regions <- sapply(main_region_overlap, function(regs) {
  if (length(regs) > 0) {
    # Get the region IDs for overlapping polygons
    region_ids <- regions$id[regs]
    
    # Find the most common region (modal value)
    region_modal <- names(sort(table(region_ids), decreasing = TRUE))[1]
    
    return(region_modal)
  } else {
    return(NA)  # No overlap case
  }
})

# Rename 'bioclim' to 'vegClimZone'
metaChm <- metaChm %>%
  rename(vegClimZone = bioclim)

# Add the region and vegClimZone labels
vegLookup <- tibble(
  vegClimZone = c(1, 2, 3, 4, 5), 
  vegClimZoneLab = c('Boreonemoral sone (BN)', 'Lavalpin sone (LA)', 'Mellomboreal sone (MB)', 'Nordboreal sone (NB)', 'Sørboreal sone (SB)')
)

regionLookup <- tibble(
  regions = c(1, 2, 3, 4, 5), 
  region_id = c('Nord-Norge', 'Midt-Norge', 'Østlandet', 'Vestlandet', 'Sørlandet')
)

# Ensure compatibility of column types for the join
metaChm <- metaChm %>%
  mutate(regions = as.numeric(regions),  # Convert 'regions' to numeric for the join
         vegClimZone = as.numeric(vegClimZone))  # Convert 'vegClimZone' to numeric for the join

# Add the region and vegClimZoneLab columns to metaChm
metaChm <- metaChm %>%
  left_join(regionLookup, by = 'regions') %>%
  left_join(vegLookup, by = 'vegClimZone')

# View the updated metaChm
head(metaChm)


```

Import forest heights per region-bioclimatic zone strata.
```{r}
cleanRegClim <- function(data){
  
  dataOut <- data %>%
    mutate(region=ifelse(region_id == 1, 'Nord-Norge',
                         ifelse(region_id == 2, 'Midt-Norge',
                                ifelse(region_id == 3, 'Østlandet',
                                       ifelse(region_id == 4, 'Vestlandet', 'Sørlandet'))))) %>%

    mutate(vegClimZone = round(vegClimZone)) %>%
    left_join(vegLookup, by = 'vegClimZone') %>%
    dplyr::select(-region_id, -vegClimZone) %>%
    drop_na(vegClimZoneLab, region)
  
  return (dataOut)
  
}


skog_region_bioclim <- read_csv("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Ida/From_GEE/vegHeights_skog_climZoneRegion.csv")


names(skog_region_bioclim)[2:4] <- c("vegClimZone","skog","region_id")


```

Add the forest height metric to each NiN polygon based on bioclimatic and region zones. 

```{r}

skog_region_bioclim <- as_tibble(skog_region_bioclim)

# Select the necessary columns from skog_region_bioclim before the join
#skog_region_bioclim_selected <- skog_region_bioclim %>%
 # dplyr::select(climZone, region, p90_height)

# Add "skog" column to metaChm by joining with skog_region_bioclim
metaChm <- metaChm %>%
  left_join(
    skog_region_bioclim, 
    by = c("vegClimZone", "regions" = "region_id")
  )

# View the updated metaChm
head(metaChm)
```
 
Import the reference height data to be used as "good" condition reference values
```{r}
refvaatmark  <- read.csv('/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/refvaatmark.csv', sep = ",")
refaapne  <- read.csv('/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/refaapne.csv', sep = ";")
refsemi  <- read.csv('/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/refsemi.csv', sep = ",")

```


Split our cnopy height dataframe into ecosystem types 
```{r}
metaAapne <- metaChm %>%
  filter(hvdksys == "Naturlig aapne")

# 2. Extract "Semi-naturlig"
metaNaturlig <- metaChm %>%
  filter(hvdksys == "Semi-naturlig")

# 3. Extract "Vaatmark"
metaVaatmark <- metaChm %>%
  filter(hvdksys == "Vaatmark")
```

Add the "good" reference values to the separate ecosystem types
```{r}

metaAapne <- metaAapne %>%
  left_join(
    refaapne, 
    by = c("vegClimZoneLab", "region_id" = "region")
  )

metaVaatmark <- metaVaatmark %>%
  left_join(
    refvaatmark, 
    by = c("vegClimZoneLab", "region_id" = "region")
  )

metaNaturlig <- metaNaturlig %>%
  left_join(
    refsemi, 
    by = c("vegClimZoneLab", "region_id" = "region")
  )
```

Filter out NA values and join back together in a dataframe
```{r}
metaNaturlig <- metaNaturlig %>%
  filter(!is.na(ref) & !is.na(skog))
metaAapne <- metaAapne %>%
  filter(!is.na(ref) & !is.na(skog))
metaVaatmark <- metaVaatmark %>%
  filter(!is.na(ref) & !is.na(skog))

metaCHM <- bind_rows(metaAapne, metaNaturlig, metaVaatmark)
```


#### 9.2.2 Calculating the index


We will calculate the gjengroing index for each population polygon using a Sigmoid scaling function.


```{r}
# Define Sigmoid scaling function
scaleSigmoid <- function(pop, ref, skog) {
  # Scale the population height between the reference (good condition) and forest height (poor condition)
  indicator_LowHigh <- (pop - ref) / (skog - ref)
  indicator_LowHigh[indicator_LowHigh < 0] <- 0
  indicator_LowHigh[indicator_LowHigh > 1] <- 1
  indicator_sigmoid <- 100.68 * (1 - exp(-5 * (indicator_LowHigh)^2.5)) / 100
  return(round(indicator_sigmoid, 4))
}
```

Now calculate the index using the median canopy height value from the meta model output
```{r}
metaCHM <- metaCHM %>%
  # Apply the scaling function to calculate the index for each polygon
  mutate(
    indexMeta = scaleSigmoid(meta_media, ref, skog),
    # Invert the index because shorter vegetation is considered better condition
    indexMeta = 1 - indexMeta
  )

# View the updated metaChm
head(metaCHM)
```
And now calculate it from the LiDAR based canopy height model
```{r}
metaCHM <- metaCHM %>%
  # Apply the scaling function to calculate the index for each polygon
  mutate(
    indexLiD = scaleSigmoid(DSM_median, ref, skog),
    # Invert the index because shorter vegetation is considered better condition
    indexLiD = 1 - indexLiD
  )

# View the updated metaChm
head(metaCHM)
```


## 10. Results

### 10.1 Validation of canopy height inference

After running inference on the meta model, we have raster tiles of canopy height estimate corresponding to our downloaded image tiles. These were mosaiced into a virtual raster for validation. Sampling for validation was performed in QGIS and consisted of creating 1000 random points within each NiN polygon used as AOI's for the tile download. These were used to sample the modeled canopy height as well as the canopy height based on LiDAR data. 

Here is an example of the output from the meta model compared to LiDAR canopy height and the image tile used to model the canopy height. Interestingly this shows encroachment in action as the vegetation heights are visibly higher in the 2020 image tile than in the 2017 LiDAR data. This is an area where new trees were planted in c. 2015.   

```{r}
#| eval: false

rgb_img <- stack("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/img/TileID_4.tif")  
pred_img <- raster("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/img/TileID_4_pred.tif") 
lidar_img <- raster("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/img/TileID_4_LiDAR_2017.tif")  

rgb_df <- as.data.frame(rgb_img, xy = TRUE)
pred_df <- as.data.frame(pred_img, xy = TRUE)
lidar_df <- as.data.frame(lidar_img, xy = TRUE)

names(rgb_df) <- c("x", "y", "red", "green", "blue")
names(pred_df) <- c("x", "y", "pred_height")
names(lidar_df) <- c("x", "y", "lidar_height")

p1 <- ggplot() +
  geom_raster(data = rgb_df, aes(x = x, y = y, fill = rgb(red, green, blue, maxColorValue = 255))) +
  scale_fill_identity() +
  coord_fixed() +  
  theme_void() +
  ggtitle("RGB Image 2020")

p2 <- ggplot() +
  geom_raster(data = pred_df, aes(x = x, y = y, fill = pred_height)) +
  scale_fill_viridis_c(option = "turbo", limits = c(0, 14), name = "Height (m)") +
  coord_fixed() +  
  theme_void() +theme(legend.position = "none")+
  ggtitle("Predicted Height 2020")


p3 <- ggplot() +
  geom_raster(data = lidar_df, aes(x = x, y = y, fill = lidar_height)) +
  scale_fill_viridis_c(option = "turbo", limits = c(-0.5, 14), name = "Height (m)") +
  coord_fixed() +  
  theme_void() +
  ggtitle("LiDAR Height 2017")

combined_plot <- (p1 + p2 + p3) + plot_layout(ncol = 3)

print(combined_plot)

```

```{r}
img <- image_read("/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/img/tileComparison.png")

plot(img)
```

Initial validation attempts showed poor performance relating to airborne images taken outside of the months of June, July and August. This was expected, since the model was trained on vegetation at peak foliation. For this reason, imagery collected outside of this month range was excluded from validation. We also filtered out points that intersected with the FKB building footprints layer, points that were in shadow (calculated by sampling the blue channel of the input RGB images and filtering out based on a threshold value). The resulting csv file after filtering contains c. 30000 points with LiDAR and Meta model based canopy heights. Future iterations of this script will automate these vaildation steps. 

```{r}
val <- read.csv('/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Andrew/meta/github/ecRxiv_encroachment/indicators/NO_GJEN_002/data/NiNpointSamples.csv')
ggplot(val, aes(x = lidmax, y = metamax)) +
  geom_point(size=2.5, alpha=0.2, stroke=0, color = 'darkgray') +  # Scatter plot
  geom_smooth(method = "lm", se = FALSE, color = "darkblue") +  # Line of best fit
  theme_minimal() +  # Clean theme
  coord_fixed(ratio = 1) +  # Square aspect ratio
  labs(title = "LiDAR vs meta modeled canopy heights", x = "LiDAR (m)", y = "Meta model (m)") +  # Labels
  # Add R-squared value to the plot
  annotate("text", x = Inf, y = Inf, label = paste("R² =", round(summary(lm(metamax ~ lidmax, data = val))$r.squared, 2)), 
           hjust = 1.1, vjust = 1.1, size = 5, color = "darkblue")+ xlim(0,30) + ylim(0,30) 
```

### 10.2 Comparison of modelled vs LiDAR gjengroing index

We can plot a scatterplot to compare LiDAR based index with the Meta based index. Note that in the subsample of NiN areas used here, most of them have a good index values. Future work to scale this over a greater number of sites should give a better spread of data for a more robust comparison. 
```{r}

ggscatterhist(
  metaCHM, x = "indexLiD", y = "indexMeta",
  color = "hvdksys", size = 3, alpha = 0.4,
  palette = c("#00AFBB", "#E7B800", "#FC4E07"),
  margin.params = list(fill = "hvdksys", color = "black", size = 0.2)
  )


ggscatterhist(
  metaCHM, x = "indexLiD", y = "indexMeta",
  color = "tilstnd", size = 3, alpha = 0.4,
  palette = c("#3b949a", "#3b9a41", "#63265f", "#9a3b94"),
  margin.params = list(fill = "hvdksys", color = "black", size = 0.2)
  )


```

They are mostly in agreement, but for this subsample of NiN areas, most have a score close to 1 (looking at the data distribution on the subplots). Visual inspection of the outliers indicates that shade within the image tiles used for inference contributed to median canopy height mismatches between the two datasets. 

We can also compare this data with a box plot:
```{r}
chm_long <- metaCHM %>%
  dplyr::select(hvdksys, indexMeta, indexLiD) %>%
  pivot_longer(cols = c(indexMeta, indexLiD), 
               names_to = "index_type", 
               values_to = "value")

# Create the bar plot with dodged bars
ggplot(chm_long, aes(x = hvdksys, y = value, fill = index_type)) +
  geom_boxplot(position = position_dodge(width = 0.8), color = "black") +
  scale_fill_manual(values = c("indexLiD" = "grey50", "indexMeta" = "white"),
                    labels = c("indexLiD" = "Index LiDAR", "indexMeta" = "Index Meta")) +
  labs(
    title = "Distribution of modelled- and LiDAR-based gjengroing index scores",
    x = "Ecoystem type",
    y = "Value",
    fill = "Index Type"
  ) +
  theme_minimal() +
   ylim(0, 1.1)

chm_long_region <- metaCHM %>%
  dplyr::select(region_id, indexMeta, indexLiD) %>%
  pivot_longer(cols = c(indexMeta, indexLiD), 
               names_to = "index_type", 
               values_to = "value")

# Create the bar plot with dodged bars
ggplot(chm_long_region, aes(x = region_id, y = value, fill = index_type)) +
  geom_boxplot(position = position_dodge(width = 0.8), color = "black") +
  scale_fill_manual(values = c("indexLiD" = "grey50", "indexMeta" = "white"),
                    labels = c("indexLiD" = "Index LiDAR", "indexMeta" = "Index Meta")) +
  labs(
    title = "Distribution of modelled- and LiDAR-based gjengroing index scores",
    x = "Ecoystem type",
    y = "Value",
    fill = "Index Type"
  ) +
  theme_minimal() +
   ylim(0, 1.1)
```

## 11. Export file

<!--# Display the code (don't execute it) or the workflow for exporting the indicator values to file. Ideally the indicator values are exported as a georeferenced shape or raster file with indicators values, reference values and errors. You can also chose to export the raw (un-normalised or unscaled variable) as a seperate product. You should not save large sptaial output data on GitHub. You can use eval=FALSE to avoid code from being executed (example below - delete if not relevant) -->

```{r export}
#| eval: false
```
